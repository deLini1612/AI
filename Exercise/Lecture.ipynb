{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Intelligent Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter:\n",
    "- Examine agents, environments, and the coupling between them\n",
    "  \n",
    "  > How well an agent can behave depends on the nature of the environment; some environments are more difficult than other\n",
    "- Categorize environment\n",
    "- Show how properties of an environment influence the design of suitable agents for that environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents and Envirionments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Agent**: **perceive** (Eng dict: = to understand something in a particular way) **environment** through **sensors** and **acting** upon that environment through **actuators**. For example:\n",
    "  - A robotic agent might have cameras and infrared range finders for sensors and various motors for actuators\n",
    "  - A software agent receives keystrokes, file contents, and network packets as sensory inputs and acts on the environment by displaying on the screen, writing files, and sending network packets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Percept** = agent’s perceptual inputs at any given instant --> agent’s **percept sequence** is the ***complete history*** of ***everything*** the agent has ever perceived"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An agent’s choice of action at any given instant can depend on the entire percept sequence observed to date, but NOT on anything it hasn’t perceived --> **an agent’s behavior is described by the agent function**\n",
    "- **Agent function**: A \"function\" that maps any given percept sequence to an action --> specifies the action taken by the agent in response to any percept sequence\n",
    "- Internally, the **agent function** for an artificial agent will be ***implemented*** by an **agent program**\n",
    "  \n",
    "  > Agent function = an abstract mathematical description; agent program = a concrete implementation, running within some physical system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good behavior: The concept of rationality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rational agent** = agent does the right thing --> ***How to measure is it a \"right thing\"*** --> It is captured by a ***performance measure*** that evaluates any given **sequence of environment states** --> performance measure will define the criterion of success\n",
    "  - **Sequence of environment states**: When an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states\n",
    "- There is not one fixed performance measure for all tasks and agents: depends on circumstances\n",
    "  \n",
    "  > it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rationality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Rational agent**: For ***each possible percept sequence***, a rational agent should select an action that is expected to **maximize its performance measure**, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has\n",
    "  \n",
    "  > **Rationality** is NOT **perfection**: Rationality maximizes ***expected*** performance, perfection maximizes ***actual*** performance\n",
    "- A rational agent need to **gather information**, **learn** as much as possible from what it perceives and should be autonomous\n",
    "  - If an agent relies on the prior knowledge of its designer rather on its own percepts --> that agent lacks **autonomy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task environment** = the \"problem\" to which rational agents are the \"solutions\" --> characteristics of the task environment directly affects the design for agent program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the task environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In designing an agent, specify the task environment *as fully as possible*\n",
    "- **PEAS** (**P**erformance measure, **E**nvironment, **A**ctuators, **S**ensors) description of task environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties of task environments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Observable**: Observable = khả năng có thể nhận biết môi trường\n",
    "  - Unobservable: Khi agent không thể nhận biết tí gì về môi trường (VD khi agent không có sensor nào)\n",
    "  - Partially observable: Khi agent có thể  nhận biết được một phần môi trường (VD như trong TH máy hút bụi ở trong sách là partially observable vì nó có thể nhận thức được tại block nó đang đứng có sạch hay không, nhưng khi nó đang đứng ở block A thì không thể nhận biết các block khác có sạch hay không mà phải di chuyển tới block đó mới biết)\n",
    "  - Fully observable: Khi agent có thể nhận biết toàn bộ trạng thái của môi trường"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Single agent** vs **multiagent**\n",
    "  - Single agent = an environment is explored by a single agent. All actions are performed by a single agent in the environment\n",
    "  - Multiagent = two or more agents are taking actions in the environment\n",
    "  \n",
    "> Difficult: Does agent A have to treat B as an agent, or can it be treated merely as an object. --> **Key distinction**: whether B’s behavior is best described as maximizing a performance measure whose value depends on agent A’s behavior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deterministic** vs **Stochastic**\n",
    "  - Deterministic = the *next state* of the environment is *completely determined* by the current state and the action executed by the agent\n",
    "  - Stochastic = The next state is totally unpredictable for the agent. So randomness exists in the environment\n",
    "- We say an environment is **uncertain** if it is not fully observable or not deterministic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Episodic** vs **Sequential**\n",
    "  - Episodic: each state is independent of each other. The action on a state has nothing to do with the next state\n",
    "  - Sequential: next state is dependent on the current action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Static** vs **dynamic**\n",
    "  - Dynamic: the environment can change while an agent is deliberating\n",
    "  - Static: the environment can't change while an agent is deliberating\n",
    "  - Semidynamic: the environment itself does not change with the passage of time but the agent’s performance score does"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Discrete** vs **continuous**\n",
    "  - Discrete: finite number of states and agents have a finite number of actions\n",
    "  - Continuous: the environment can have an infinite number of states. So the possibilities of taking an action are also infinite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Known** vs **unknown**: Strictly speaking, this distinction refers not to the environment itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment.\n",
    "  - Known: the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given\n",
    "  - Unknown: not known --> the agent will have to learn how it works in order to make good decisions.\n",
    "\n",
    "> **KNOWN != OBSERVABLE**. For example: in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> The hardest case is ***partially observable, multiagent, stochastic, sequential, dynamic, continuous, and unknown***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The job of AI is to design an **agent program** that implements the agent function --> the mapping from percepts to actions --> this program will run on an architecture (some sort of computing device with physical sensors and actuators) --> program have to be appropriate fot architecture \n",
    "\n",
    "> ***agent = architecture + program***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent programs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent program = take the **current percept** as **input** from the sensors and **return** an **action** to the actuators\n",
    "  \n",
    "  > **Agent function**, which takes the entire percept history. **Agent program** takes the current percept as input (because nothing more is available from the environment; if the agent’s actions need to depend on the entire percept sequence, the agent will have to remember the percepts)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on methods for selecting actions --> **4** basic kinds of agent programs (all these can be turned into **learning agents**)\n",
    "  1. Simple reflex agents\n",
    "  2. Model-based reflex agents\n",
    "  3. Goal-based agents\n",
    "  4. Utility-based agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple reflex agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplest kind of agent, just select actions based on the ***current*** percept, ignoring the rest of the percept history\n",
    "- Simple reflex agent acts according to a rule whose condition matches the current state, as defined by the percept\n",
    "- **Condition-action rule**: mối quan hệ nguyên nhân, kết quả. VD: if car-in-front-is-braking then initiate-braking; if dirty then suck,...\n",
    "- Condition–action rules allow the agent to make the connection from percept to actio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim: Simple reflex agent:\n",
    "    - Simple\n",
    "    - Limited intelligence: agent will work only if the correct decision can be made based on only the current percept --> only if the environment is ***fully observable***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model-based reflex agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The agent maintain some sort of **internal state** that depends on the percept history --> reflects at least some of the unobserved aspects of the current state \n",
    "- Updating this internal state information as time goes by requires information about:\n",
    "  - How the world evolves independently of the agent\n",
    "  - How the agent’s own actions affect the world\n",
    "\n",
    "--> This knowledge about \"how the world works\" = **model** of the world --> Agent uses such a model called **model-based agent**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal-based agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As well as a current state description, the agent needs some sort of **goal** information that describes situations that are desirable\n",
    "- **Goal-based agent** = agent program can combine this (goal information) with the model (same as in model-based reflex agent) to choose actions that **achieve the goal**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claim: Although the goal-based agent appears ***less efficient***, it is ***more flexible*** because the knowledge that supports its decisions is represented explicitly and can be modified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility-based agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goals just provide a crude binary distinction between “happy” and “unhappy” states --> A more general performance measure should allow to determine exactly how \"happy\" they would make the agent --> **utility**\n",
    "  - For example, many action sequences will get the taxi to its destination (thereby achieving the goal) but some are quicker, safer, more reliable, or cheaper than others\n",
    "- **Utility-based agent**: An agent chooses actions to maximize its **expected utility**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A learning agent can be divided into **four** conceptual components\n",
    "  - Learning element: Responsible for making improvements\n",
    "  - Performance element: Responsible for selecting external actions (takes in percepts and decides on actions)\n",
    "  - Problem generator: Responsible for suggesting actions that will lead to new and informative experience\n",
    "  - Critic: Tells the learning element how well the agent is doing with respect to a fixed performance standard\n",
    "\n",
    "--> Learning element uses ***feedback*** from the **critic** on ***how the agent is doing*** and determines how the ***performance element*** should be modified to do better in the future\n",
    "\n",
    "--> The design of the learning element depends very much on the design of the performance element"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Solving Problems By Searching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem-solving agent** = one kind of **goal-based agent**\n",
    "  - Problem-solving agents use **atomic**representations: states of the world are considered as wholes\n",
    "- **Uninformed** search algorithms: algorithms that are given no information about the problem other than its definition\n",
    "- **Informed** search algorithms: given some guidance on where to look for solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-solving agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First step of problem solving: **Goal formulation** --> based on the current situation and the agent’s performance measure\n",
    "- **Problem formulation** = the process of deciding what actions and states to consider, given a goal\n",
    "\n",
    "> An agent with several immediate options of unknown value can decide what to do by first examining future actions that eventually lead to states of known value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume environment is ***observable, discrete, known, deterministic***\n",
    "  > Under these assumptions, the solution to any problem is a fixed sequence of actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Search** = The process of looking for a sequence of actions that reaches the goal\n",
    "  - Input: problem\n",
    "  - Output: solution in the form of an action --> **execution** phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> After ***formulating a goal and a problem*** to solve, the agent calls a ***search procedure*** to solve it. It then uses the solution to guide its actions, doing the first action of the sequence and then removing that step from the sequence. Once the solution has been ***executed***, the agent will ***formulate a new goal*** (Book: Figure 3.1 page 67 3rd edition)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While the agent is executing the solution sequence it ***ignores its percepts*** when choosing an action because it knows in advance what they will be --> **open-loop** system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well-defined problems and solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **problem** can be defined by 5 components:\n",
    "    1. **Initial state**\n",
    "    2. A description of the possible **actions** **applicable** in state *s* --> specified by function *ACTIONS(s)*\n",
    "    3. **Transition model** :a description of what action doses --> specified by a function *RESULT(s,a)* that return the state that results from doing action *a* in state *s*\n",
    "        - Together, ***initial state***, ***actions*** ***transition model*** implicitly define the **state space** of the problem (the set of all states reachable from the initial state by any sequence of actions)\n",
    "        - The state space forms a directed network or **graph**\n",
    "          - Nodes: state\n",
    "          - Link between nodes: action\n",
    "          - Path: a sequence of states connected by a sequence of actions\n",
    "        - Sometimes, we use ***successor function*** (return set of all reachable state from a given state by a single action) instead of separate *ACTIONS* and *RESULT* function\n",
    "    4. **Goal test**: determines whether a given state is a goal state\n",
    "    5. **Path cost**: assigns a numeric cost to each path --> The problem-solving agent chooses a cost function that reflects its own performance measure\n",
    "        - Assume that the cost of a path = the sum of the costs of the individual actions along the path\n",
    "        - **Step cost** of taking action *a* in state *s* to reach state *s'* is denoted by *c(s,a,s')*\n",
    "  \n",
    "- **Solution** = an action sequence that leads from the initial state to a goal state\n",
    "- **Optimal solution** = the lowest path cost among all solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulating problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real world is absurdly complex ---> for problem solving: need abstraction\n",
    "- **Attraction** = the process of removing detail from a representation\n",
    "- The abstraction is useful if carrying out each of the actions in the solution is easier than the original problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem types\n",
    "  - **Single-state** problem: deterministic, fully observable\n",
    "    - Agent knows exactly which state it will be in\n",
    "    - Solution is a sequence\n",
    "  - **Conformant** problem: Non-observable\n",
    "    - Agent may have no idea where it is\n",
    "    - Solution (if any) is a sequence\n",
    "  - **Contingency** problem: Nondeterministic and/or partially observable\n",
    "    - Percepts provide *new* information about current state\n",
    "    - Solution is a *contingent plan* or a *policy*\n",
    "    - Often interleave search, execution\n",
    "  - **Exploration** problem: Unknown state space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: P, NP, NP-Hard, NP-Complete problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **P**: problems that **can be solved in polynomial time** by a deterministic algorithms\n",
    "  - they are solvable in O(p(n)), where p(n) is a polynomial on n. Ex: O(n^2), O(1), O(n*lg(n))\n",
    "\n",
    "- **NP**: problems that can be solved in polynomial time **on a nondeterministic** machine (or with a nondeterministic algorithm)\n",
    "  - A *nondeterministic* machine is one that can *\"guess\"* the right answer/solution\n",
    "  --> NP can also be thought of as the class of problems “whose solutions can be ***verified in polynomial time***\n",
    "\n",
    "> P is a subset of NP\n",
    "\n",
    "- **NP-complete**: a class of all problems `X` ***in NP*** for which it is possible to *reduce any other NP problem `Y` to `X`* in ***polynomial time***\n",
    "  --> we can solve `Y` quickly if we know how to solve `X` quickly\n",
    "\n",
    "- **NP-hard**: a class of all problems `X` that there is an ***NP-complete*** problem `Y` reducible to `X` in polynomial time\n",
    "  --> problems that are at least as hard as the NP-complete problems (do not have to be in NP)\n",
    "\n",
    "> NP-complete is the intersection of NP and NP-hard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching For Solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A solution is an action sequence --> ***search algorithms*** work by considering various possible action sequences\n",
    "- The possible action sequences starting at the initial state form a **search tree** with:\n",
    "  - Root: the initial state\n",
    "  - Branches: actions\n",
    "  - Nodes: states in the state space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To avoid exploring redundant path: ***remembers*** every expanded node. Newly generated nodes that *match previously generated* nodes can be ***discarded*** instead of being added to the frontier -> called *GRAPH-SEARCH* algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure for search algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each node `n` of the tree, we have a structure that contains 4 components:\n",
    "    1. `n.STATE`: state in the state space to which the node corresponds\n",
    "    2. `n.PARENT`\n",
    "    3. `n.ACTION`: action that was applied to the parent to generate the node\n",
    "    4. `n.PATH-COST`: cost, traditionally denoted by `g(n)`, of the path from the ***initial state*** to the node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using **queue** to store nodes. Operations on a queue:\n",
    "  - `EMPTY?(queue)`\n",
    "  - `POP(queue)`\n",
    "  - `INSERT(queue)`\n",
    "- Types of queue:\n",
    "  - FIFO: First-in, first-out\n",
    "  - LIFO (aka **stack**): Last-in, first-out\n",
    "  - Priority queue: pops the element of the queue with the ***highest priority*** according to some ordering function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring problem-solving performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can evaluate an algorithm’s performance in **4 ways**:\n",
    "    1. Completeness: Is the algorithm guaranteed to find a solution when there is one?\n",
    "    2. Optimally: Does the strategy find the optimal solution?\n",
    "    3. Time complexity\n",
    "    4. Space complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When graph is an ***explicit*** data structure, *time and space complexity* are always considered with respect to some measure of the ***problem difficulty***: |V| + |E| where C = set of vertices (nodes), E = set of edges (links)\n",
    "- In AI, the graph is often represented ***implicitly*** --> complexity is expressed in terms of **3 quantities**\n",
    "  - b = *maximum* **branching factor** (aka number of successors of a node) of the search tree\n",
    "  - d = **depth** of the *least-cost* solution\n",
    "  - m = *maximum* depth of the state space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Total cost** = Search cost + Path cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uninformed Search Strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Uninformed search** (aka **blind search**): strategies use only the information available in the problem definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth-first search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform-cost search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-first search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-limited search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative deepening depth-first search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing uninformed search strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Informed search** (aka **heuristic search**) = trategies that know whether one non-goal state is “more promising” than another"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
